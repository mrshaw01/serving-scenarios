{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1805e2",
   "metadata": {},
   "source": [
    "# Tensormesh Smoke Test Notebook\n",
    "\n",
    "This notebook performs a minimal request against a Tensormesh-deployed model through an OpenAI-compatible endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911fd68",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Create `.env` from `.env.example`.\n",
    "- Set `OPENAI_BASE_URL`, `OPENAI_API_KEY`, `TENSORMESH_USER_ID`, and `OPENAI_MODEL`.\n",
    "- Install dependency if needed: `pip install openai`.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load values from `.env`.\n",
    "2. Validate required environment variables.\n",
    "3. Send one chat completion request.\n",
    "4. Print response text, metadata, and raw JSON payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63baed88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T12:32:17.339501Z",
     "iopub.status.busy": "2026-02-26T12:32:17.339229Z",
     "iopub.status.idle": "2026-02-26T12:32:17.349229Z",
     "shell.execute_reply": "2026-02-26T12:32:17.348102Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def load_dotenv(dotenv_path: Path = Path('.env')) -> None:\n",
    "    if not dotenv_path.exists():\n",
    "        return\n",
    "    for raw_line in dotenv_path.read_text(encoding='utf-8').splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line or line.startswith('#') or '=' not in line:\n",
    "            continue\n",
    "        key, value = line.split('=', 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip('\\\"').strip(\"'\")\n",
    "        os.environ.setdefault(key, value)\n",
    "\n",
    "\n",
    "def require_env(name: str) -> str:\n",
    "    value = os.environ.get(name)\n",
    "    if not value:\n",
    "        raise RuntimeError(f'Missing required env var: {name}')\n",
    "    return value\n",
    "\n",
    "\n",
    "def mask_secret(secret: str, keep: int = 4) -> str:\n",
    "    if len(secret) <= keep:\n",
    "        return '*' * len(secret)\n",
    "    return '*' * (len(secret) - keep) + secret[-keep:]\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4d8b87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T12:32:17.351729Z",
     "iopub.status.busy": "2026-02-26T12:32:17.351504Z",
     "iopub.status.idle": "2026-02-26T12:32:17.356356Z",
     "shell.execute_reply": "2026-02-26T12:32:17.355410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration\n",
      "  OPENAI_BASE_URL   : https://external.nebius.tensormesh.ai/v1\n",
      "  OPENAI_MODEL      : openai/gpt-oss-20b\n",
      "  TENSORMESH_USER_ID: 60b01a41-c8a4-4234-80a5-ef18309b560c\n",
      "  OPENAI_API_KEY    : *******************************fdMX\n"
     ]
    }
   ],
   "source": [
    "base_url = require_env('OPENAI_BASE_URL').rstrip('/')\n",
    "api_key = require_env('OPENAI_API_KEY')\n",
    "user_id = require_env('TENSORMESH_USER_ID')\n",
    "model = require_env('OPENAI_MODEL')\n",
    "\n",
    "if not base_url.endswith('/v1'):\n",
    "    print('Note: OPENAI_BASE_URL usually ends with /v1')\n",
    "\n",
    "print('Configuration')\n",
    "print(f'  OPENAI_BASE_URL   : {base_url}')\n",
    "print(f'  OPENAI_MODEL      : {model}')\n",
    "print(f'  TENSORMESH_USER_ID: {user_id}')\n",
    "print(f'  OPENAI_API_KEY    : {mask_secret(api_key)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b205dbec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T12:32:17.358343Z",
     "iopub.status.busy": "2026-02-26T12:32:17.358189Z",
     "iopub.status.idle": "2026-02-26T12:32:17.556980Z",
     "shell.execute_reply": "2026-02-26T12:32:17.556538Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        'Failed to import openai package. Install with: pip install openai'\n",
    "    ) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049c043e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T12:32:17.558318Z",
     "iopub.status.busy": "2026-02-26T12:32:17.558238Z",
     "iopub.status.idle": "2026-02-26T12:32:19.925084Z",
     "shell.execute_reply": "2026-02-26T12:32:19.924266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke test request completed in 2346.8 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url,\n",
    "    default_headers={'X-User-Id': user_id},\n",
    ")\n",
    "\n",
    "request_messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a helpful assistant.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Write a short haiku about cloud compute.'\n",
    "    },\n",
    "]\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=request_messages,\n",
    "    temperature=0.7,\n",
    ")\n",
    "latency_ms = (time.perf_counter() - t0) * 1000\n",
    "print(f'Smoke test request completed in {latency_ms:.1f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9c6f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T12:32:19.927489Z",
     "iopub.status.busy": "2026-02-26T12:32:19.927303Z",
     "iopub.status.idle": "2026-02-26T12:32:19.930729Z",
     "shell.execute_reply": "2026-02-26T12:32:19.929989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant response\n",
      "================================================================================\n",
      "Virtual clouds hum  \n",
      "Code rains over cloudy sky  \n",
      "Data flows freely\n"
     ]
    }
   ],
   "source": [
    "assistant_text = response.choices[0].message.content or ''\n",
    "print('Assistant response')\n",
    "print('=' * 80)\n",
    "print(assistant_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6437eb9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T12:32:19.932430Z",
     "iopub.status.busy": "2026-02-26T12:32:19.932297Z",
     "iopub.status.idle": "2026-02-26T12:32:19.935382Z",
     "shell.execute_reply": "2026-02-26T12:32:19.934838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response summary\n",
      "  id    : chatcmpl-aa125713-7525-4120-b053-a36a8f64f583\n",
      "  model : openai/gpt-oss-20b\n",
      "  usage : CompletionUsage(completion_tokens=381, prompt_tokens=88, total_tokens=469, completion_tokens_details=None, prompt_tokens_details=None)\n"
     ]
    }
   ],
   "source": [
    "usage = getattr(response, 'usage', None)\n",
    "print('Response summary')\n",
    "print(f\"  id    : {getattr(response, 'id', None)}\")\n",
    "print(f\"  model : {getattr(response, 'model', None)}\")\n",
    "print(f\"  usage : {usage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137f03bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T12:32:19.937156Z",
     "iopub.status.busy": "2026-02-26T12:32:19.937018Z",
     "iopub.status.idle": "2026-02-26T12:32:19.939905Z",
     "shell.execute_reply": "2026-02-26T12:32:19.939302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response payload\n",
      "================================================================================\n",
      "{\n",
      "  \"id\": \"chatcmpl-aa125713-7525-4120-b053-a36a8f64f583\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"Virtual clouds hum  \\nCode rains over cloudy sky  \\nData flows freely\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": \"User wants a short haiku about cloud compute. Haiku is 5-7-5 syllable structure. Must mention cloud compute. Let's craft: \\\"Virtual clouds hum, code rains over sky, data flows free.\\\" Count syllables: Virtual(3) clouds(1) hum(1) =5? Let's count: Virtual (3), clouds(1), hum(1) =5. Next line: code(1) rains(1) over(2) sky(1) =5? Wait 1+1+2+1 =5. But need 7. Need 7. \\\"code rains over cloudy sky\\\" Count: code(1), rains(1), over(2), cloudy(2), sky(1) =7. Good. Third line: data flows free. Count: data(2), flows(1), free(1)=4. Need 5. Add \\\"swiftly\\\" maybe. \\\"data flows swiftly free\\\" Count: data(2), flows(1), swiftly(2), free(1)=6. Too many. Maybe \\\"data flows, free\\\" Count: data(2), flows(1), free(1)=4. Need 1 more. \\\"data flows, freely\\\" Count: data(2), flows(1), freely(2)=5. Good. So haiku:\\n\\nVirtual clouds hum  \\nCode rains over cloudy sky  \\nData flows freely\\n\\nCheck syllable counts: Virtual(3), clouds(1), hum(1)=5. Code(1), rains(1), over(2), cloudy(2), sky(1)=7. Data(2), flows(1), freely(2)=5. Good. Provide as answer.\",\n",
      "        \"reasoning_content\": \"User wants a short haiku about cloud compute. Haiku is 5-7-5 syllable structure. Must mention cloud compute. Let's craft: \\\"Virtual clouds hum, code rains over sky, data flows free.\\\" Count syllables: Virtual(3) clouds(1) hum(1) =5? Let's count: Virtual (3), clouds(1), hum(1) =5. Next line: code(1) rains(1) over(2) sky(1) =5? Wait 1+1+2+1 =5. But need 7. Need 7. \\\"code rains over cloudy sky\\\" Count: code(1), rains(1), over(2), cloudy(2), sky(1) =7. Good. Third line: data flows free. Count: data(2), flows(1), free(1)=4. Need 5. Add \\\"swiftly\\\" maybe. \\\"data flows swiftly free\\\" Count: data(2), flows(1), swiftly(2), free(1)=6. Too many. Maybe \\\"data flows, free\\\" Count: data(2), flows(1), free(1)=4. Need 1 more. \\\"data flows, freely\\\" Count: data(2), flows(1), freely(2)=5. Good. So haiku:\\n\\nVirtual clouds hum  \\nCode rains over cloudy sky  \\nData flows freely\\n\\nCheck syllable counts: Virtual(3), clouds(1), hum(1)=5. Code(1), rains(1), over(2), cloudy(2), sky(1)=7. Data(2), flows(1), freely(2)=5. Good. Provide as answer.\"\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1772109138,\n",
      "  \"model\": \"openai/gpt-oss-20b\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 381,\n",
      "    \"prompt_tokens\": 88,\n",
      "    \"total_tokens\": 469,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "raw = response.model_dump() if hasattr(response,\n",
    "                                       'model_dump') else dict(response)\n",
    "print('Raw response payload')\n",
    "print('=' * 80)\n",
    "print(json.dumps(raw, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
