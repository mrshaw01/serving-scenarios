{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1805e2",
   "metadata": {},
   "source": [
    "# Tensormesh Smoke Test Notebook\n",
    "\n",
    "This notebook performs a minimal request against a Tensormesh-deployed model through an OpenAI-compatible endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911fd68",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Create `.env` from `.env.example`.\n",
    "- Set `OPENAI_BASE_URL`, `OPENAI_API_KEY`, `TENSORMESH_USER_ID`, and `OPENAI_MODEL`.\n",
    "- Install dependency if needed: `pip install openai`.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load values from `.env`.\n",
    "2. Validate required environment variables.\n",
    "3. Send one chat completion request.\n",
    "4. Print response text, metadata, and raw JSON payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63baed88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T08:29:31.690975Z",
     "iopub.status.busy": "2026-02-25T08:29:31.690737Z",
     "iopub.status.idle": "2026-02-25T08:29:31.701658Z",
     "shell.execute_reply": "2026-02-25T08:29:31.701238Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def load_dotenv(dotenv_path: Path = Path('.env')) -> None:\n",
    "    if not dotenv_path.exists():\n",
    "        return\n",
    "    for raw_line in dotenv_path.read_text(encoding='utf-8').splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line or line.startswith('#') or '=' not in line:\n",
    "            continue\n",
    "        key, value = line.split('=', 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip('\\\"').strip(\"'\")\n",
    "        os.environ.setdefault(key, value)\n",
    "\n",
    "\n",
    "def require_env(name: str) -> str:\n",
    "    value = os.environ.get(name)\n",
    "    if not value:\n",
    "        raise RuntimeError(f'Missing required env var: {name}')\n",
    "    return value\n",
    "\n",
    "\n",
    "def mask_secret(secret: str, keep: int = 4) -> str:\n",
    "    if len(secret) <= keep:\n",
    "        return '*' * len(secret)\n",
    "    return '*' * (len(secret) - keep) + secret[-keep:]\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4d8b87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T08:29:31.703849Z",
     "iopub.status.busy": "2026-02-25T08:29:31.703663Z",
     "iopub.status.idle": "2026-02-25T08:29:31.707365Z",
     "shell.execute_reply": "2026-02-25T08:29:31.706965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration\n",
      "  OPENAI_BASE_URL   : https://external.nebius.tensormesh.ai/v1\n",
      "  OPENAI_MODEL      : openai/gpt-oss-20b\n",
      "  TENSORMESH_USER_ID: 60b01a41-c8a4-4234-80a5-ef18309b560c\n",
      "  OPENAI_API_KEY    : *******************************fdMX\n"
     ]
    }
   ],
   "source": [
    "base_url = require_env('OPENAI_BASE_URL').rstrip('/')\n",
    "api_key = require_env('OPENAI_API_KEY')\n",
    "user_id = require_env('TENSORMESH_USER_ID')\n",
    "model = require_env('OPENAI_MODEL')\n",
    "\n",
    "if not base_url.endswith('/v1'):\n",
    "    print('Note: OPENAI_BASE_URL usually ends with /v1')\n",
    "\n",
    "print('Configuration')\n",
    "print(f'  OPENAI_BASE_URL   : {base_url}')\n",
    "print(f'  OPENAI_MODEL      : {model}')\n",
    "print(f'  TENSORMESH_USER_ID: {user_id}')\n",
    "print(f'  OPENAI_API_KEY    : {mask_secret(api_key)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b205dbec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T08:29:31.709155Z",
     "iopub.status.busy": "2026-02-25T08:29:31.709026Z",
     "iopub.status.idle": "2026-02-25T08:29:31.882338Z",
     "shell.execute_reply": "2026-02-25T08:29:31.881998Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        'Failed to import openai package. Install with: pip install openai'\n",
    "    ) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049c043e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T08:29:31.883640Z",
     "iopub.status.busy": "2026-02-25T08:29:31.883564Z",
     "iopub.status.idle": "2026-02-25T08:29:34.950047Z",
     "shell.execute_reply": "2026-02-25T08:29:34.949565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke test request completed in 3051.9 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url,\n",
    "    default_headers={'X-User-Id': user_id},\n",
    ")\n",
    "\n",
    "request_messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a helpful assistant.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Write a short haiku about cloud compute.'\n",
    "    },\n",
    "]\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=request_messages,\n",
    "    temperature=0.7,\n",
    ")\n",
    "latency_ms = (time.perf_counter() - t0) * 1000\n",
    "print(f'Smoke test request completed in {latency_ms:.1f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9c6f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T08:29:34.951995Z",
     "iopub.status.busy": "2026-02-25T08:29:34.951831Z",
     "iopub.status.idle": "2026-02-25T08:29:34.954281Z",
     "shell.execute_reply": "2026-02-25T08:29:34.953942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant response\n",
      "================================================================================\n",
      "Sky of bits hums bright  \n",
      "Virtual cores hum in silence  \n",
      "Data flows like mist\n"
     ]
    }
   ],
   "source": [
    "assistant_text = response.choices[0].message.content or ''\n",
    "print('Assistant response')\n",
    "print('=' * 80)\n",
    "print(assistant_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6437eb9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T08:29:34.955812Z",
     "iopub.status.busy": "2026-02-25T08:29:34.955682Z",
     "iopub.status.idle": "2026-02-25T08:29:34.958012Z",
     "shell.execute_reply": "2026-02-25T08:29:34.957709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response summary\n",
      "  id    : chatcmpl-1e1ffc55-2f0b-4cc3-83b7-5f480684e3ed\n",
      "  model : openai/gpt-oss-20b\n",
      "  usage : CompletionUsage(completion_tokens=521, prompt_tokens=88, total_tokens=609, completion_tokens_details=None, prompt_tokens_details=None)\n"
     ]
    }
   ],
   "source": [
    "usage = getattr(response, 'usage', None)\n",
    "print('Response summary')\n",
    "print(f\"  id    : {getattr(response, 'id', None)}\")\n",
    "print(f\"  model : {getattr(response, 'model', None)}\")\n",
    "print(f\"  usage : {usage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137f03bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T08:29:34.959429Z",
     "iopub.status.busy": "2026-02-25T08:29:34.959313Z",
     "iopub.status.idle": "2026-02-25T08:29:34.961768Z",
     "shell.execute_reply": "2026-02-25T08:29:34.961475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response payload\n",
      "================================================================================\n",
      "{\n",
      "  \"id\": \"chatcmpl-1e1ffc55-2f0b-4cc3-83b7-5f480684e3ed\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"Sky of bits hums bright  \\nVirtual cores hum in silence  \\nData flows like mist\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": \"We need a haiku 5-7-5 syllables. Short haiku about cloud compute. Let's craft: \\\"Sky of bits hums, / virtual cores in silence, / data flows like mist.\\\" Count syllables: Sky(1) of(1) bits(1) hums(1) =4? Actually \\\"Sky of bits hums\\\" = 1+1+1+1 =4. Need 5. Maybe \\\"Sky of bits hums bright\\\". That's 5: Sky(1) of(1) bits(1) hums(1) bright(1) =5. Next line: \\\"virtual cores in silence\\\" count: virt(1) ual(1) cores(1) in(1) si(1) lence(1) =6? Let's count: virt (1), ual (1) =2, cores (1)=3, in(1)=4, si(1)=5, lence(1)=6. So 6, need 7. Add one more: \\\"quiet\\\" maybe: \\\"virtual cores in quiet silence\\\" count: virt(1) ual(1)=2, cores(1)=3, in(1)=4, quiet(2) =6, silence(2) =8. Too many. Let's find a 7-syllable line: \\\"virtual cores hum in silence\\\". Count: virt(1) ual(1)=2, cores(1)=3, hum(1)=4, in(1)=5, si(1)=6, lence(1)=7. Good. Third line: \\\"data flows like mist\\\" count: data(2) flows(1) like(1) mist(1)=5. Need 5. Good. So haiku:\\n\\nSky of bits hums bright\\nVirtual cores hum in silence\\nData flows like mist\\n\\nCheck syllable counts: first: Sky(1) of(1) bits(1) hums(1) bright(1) =5. second: virt(1) ual(1)=2, cores(1)=3, hum(1)=4, in(1)=5, si(1)=6, lence(1)=7. third: data(2) flows(1) like(1) mist(1)=5. Good.\\n\\nLet's output.\",\n",
      "        \"reasoning_content\": \"We need a haiku 5-7-5 syllables. Short haiku about cloud compute. Let's craft: \\\"Sky of bits hums, / virtual cores in silence, / data flows like mist.\\\" Count syllables: Sky(1) of(1) bits(1) hums(1) =4? Actually \\\"Sky of bits hums\\\" = 1+1+1+1 =4. Need 5. Maybe \\\"Sky of bits hums bright\\\". That's 5: Sky(1) of(1) bits(1) hums(1) bright(1) =5. Next line: \\\"virtual cores in silence\\\" count: virt(1) ual(1) cores(1) in(1) si(1) lence(1) =6? Let's count: virt (1), ual (1) =2, cores (1)=3, in(1)=4, si(1)=5, lence(1)=6. So 6, need 7. Add one more: \\\"quiet\\\" maybe: \\\"virtual cores in quiet silence\\\" count: virt(1) ual(1)=2, cores(1)=3, in(1)=4, quiet(2) =6, silence(2) =8. Too many. Let's find a 7-syllable line: \\\"virtual cores hum in silence\\\". Count: virt(1) ual(1)=2, cores(1)=3, hum(1)=4, in(1)=5, si(1)=6, lence(1)=7. Good. Third line: \\\"data flows like mist\\\" count: data(2) flows(1) like(1) mist(1)=5. Need 5. Good. So haiku:\\n\\nSky of bits hums bright\\nVirtual cores hum in silence\\nData flows like mist\\n\\nCheck syllable counts: first: Sky(1) of(1) bits(1) hums(1) bright(1) =5. second: virt(1) ual(1)=2, cores(1)=3, hum(1)=4, in(1)=5, si(1)=6, lence(1)=7. third: data(2) flows(1) like(1) mist(1)=5. Good.\\n\\nLet's output.\"\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1772008172,\n",
      "  \"model\": \"openai/gpt-oss-20b\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 521,\n",
      "    \"prompt_tokens\": 88,\n",
      "    \"total_tokens\": 609,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "raw = response.model_dump() if hasattr(response,\n",
    "                                       'model_dump') else dict(response)\n",
    "print('Raw response payload')\n",
    "print('=' * 80)\n",
    "print(json.dumps(raw, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
